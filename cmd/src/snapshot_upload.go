package main

import (
	"context"
	"flag"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"slices"
	"strings"

	"cloud.google.com/go/storage"
	"github.com/sourcegraph/conc/pool"
	"github.com/sourcegraph/sourcegraph/lib/errors"
	"github.com/sourcegraph/sourcegraph/lib/output"
	"google.golang.org/api/option"

	"github.com/sourcegraph/src-cli/internal/pgdump"
)

// Package-level variables
const srcSnapshotDir = "./src-snapshot"

// summaryFile on its own, as it gets handled a little differently
const summaryFile = "summary.json"
var srcSnapshotSummaryPath = filepath.Join(srcSnapshotDir, summaryFile)

// listOfValidFiles defines the valid snapshot filenames (with extensions) that can be uploaded
var listOfValidFiles = []string{
	"codeinsights.sql",
	"codeintel.sql",
	"pgsql.sql",
	summaryFile,
}

// Define types
type uploadArgs struct {
	bucketName      string
	credentialsPath string
	filterSQL       bool
	filesToUpload   []string
}

// Google Cloud Storage upload client
type gcsClient struct {
	ctx           context.Context
	out           *output.Output
	storageClient *storage.Client
}

// uploadFile represents a file opened for upload
type uploadFile struct {
	file      *os.File
	stat      os.FileInfo
	filterSQL bool // Whether to filter incompatible SQL statements during upload, true for database files, false for summary file
}


func init() {
	usage := fmt.Sprintf(`'src snapshot upload' uploads instance snapshot contents generated by 'src snapshot databases' and 'src snapshot summary' to the designated bucket.

USAGE
	src snapshot upload -bucket=$MIGRATION_BUCKET_NAME -credentials=./migration_private_key.json [-file=$FILE]

BUCKET
	In general, a Google Cloud Storage bucket and relevant credentials will be provided by Sourcegraph when using this functionality to share a snapshot with Sourcegraph.

FILE
	Optional: Specify files to upload as a comma-delimited list (with file extensions). Valid values: %s. Default: All files.
	Examples: -file=summary.json,pgsql.sql or -file=codeintel.sql
`, strings.Join(listOfValidFiles, ", "))

	flagSet := flag.NewFlagSet("upload", flag.ExitOnError)
	bucketName := flagSet.String("bucket", "", "destination Cloud Storage bucket name")
	credentialsPath := flagSet.String("credentials", "", "JSON credentials file for Google Cloud service account")
	fileFilter := flagSet.String("file", strings.Join(listOfValidFiles, ","), "comma-delimited list of files to upload")
	filterSQL := flagSet.Bool("filter-sql", true, "filter incompatible SQL statements from database dumps for import to Google Cloud SQL")

	// Register this command with the parent 'src snapshot' command.
	// The parent snapshot.go command runs all registered subcommands via snapshotCommands.run().
	// This self-registration pattern allows subcommands to automatically register themselves
	// when their init() functions run, without requiring a central registry file.
	snapshotCommands = append(snapshotCommands, &command{
		flagSet:   flagSet,
		handler:   snapshotUploadHandler(flagSet, bucketName, credentialsPath, filterSQL, fileFilter),
		usageFunc: func() { fmt.Fprint(flag.CommandLine.Output(), usage) },
	})
}

// Handler function to keep init() succinct
func snapshotUploadHandler(flagSet *flag.FlagSet, bucketName, credentialsPath *string, filterSQL *bool, fileFilter *string) func([]string) error {
	return func(args []string) error {
		if err := flagSet.Parse(args); err != nil {
			return err
		}

		// Validate and parse inputs into an uploadArgs-type object
		uploadArgs, err := validateUploadInputs(*bucketName, *credentialsPath, *fileFilter, *filterSQL)
		if err != nil {
			return err
		}

		// Create client
		client, err := createGcsClient(flagSet, uploadArgs.credentialsPath)
		if err != nil {
			return err
		}

		// Open files and create progress bars
		openedFiles, progressBars, err := openFilesAndCreateProgressBars(uploadArgs)
		if err != nil {
			return err
		}

		// Upload files to bucket
		return uploadFilesToBucket(client, uploadArgs, openedFiles, progressBars)
	}
}

// Validate user inputs, and convert them to an object of type uploadArgs
func validateUploadInputs(bucketName, credentialsPath, fileFilter string, filterSQL bool) (*uploadArgs, error) {
	if bucketName == "" {
		return nil, errors.New("-bucket required")
	}
	if credentialsPath == "" {
		return nil, errors.New("-credentials required")
	}

	filesToUpload, err := parseFileFilter(fileFilter)
	if err != nil {
		return nil, err
	}

	return &uploadArgs{
		bucketName:      bucketName,
		credentialsPath: credentialsPath,
		filterSQL:       filterSQL,
		filesToUpload:   filesToUpload,
	}, nil
}

// Parse the --file arg values, and return a list of strings of the files to upload
func parseFileFilter(fileFilter string) ([]string, error) {

	// Default: all files
	if fileFilter == "" {
		return listOfValidFiles, nil
	}

	var filesToUpload []string

	// Parse comma-delimited list
	for _, part := range strings.Split(fileFilter, ",") {

		// Trim whitespace
		filename := strings.TrimSpace(part)

		// Validate against list of valid files
		if !slices.Contains(listOfValidFiles, filename) {
			return nil, errors.Newf("invalid -file value %q. Valid values: %s", part, strings.Join(listOfValidFiles, ", "))
		}

		filesToUpload = append(filesToUpload, filename)
	}

	// Sort files alphabetically for consistent ordering
	slices.Sort(filesToUpload)

	// Remove duplicates (works on sorted slices by removing adjacent duplicates)
	filesToUpload = slices.Compact(filesToUpload)

	return filesToUpload, nil
}

func createGcsClient(flagSet *flag.FlagSet, credentialsPath string) (*gcsClient, error) {

	ctx := context.Background()
	out := output.NewOutput(flagSet.Output(), output.OutputOpts{Verbose: *verbose})

	// https://pkg.go.dev/cloud.google.com/go/storage#section-readme
	client, err := storage.NewClient(ctx, option.WithCredentialsFile(credentialsPath))

	if err != nil {
		return nil, errors.Wrap(err, "create Google Cloud Storage client")
	}

	return &gcsClient{
		ctx:           ctx,
		out:           out,
		storageClient: client,
	}, nil
}

// openFilesAndCreateProgressBars opens selected snapshot files from disk and creates progress bars for UI display.
// Returns arrays of uploadFile and progress bars (aligned by index).
func openFilesAndCreateProgressBars(args *uploadArgs) ([]uploadFile, []output.ProgressBar, error) {
	var (
		openedFiles  []uploadFile         // Files opened from disk, ready for upload (aligned with progressBars)
		progressBars []output.ProgressBar // Progress bars for UI (aligned with openedFiles)
	)

	// addFile opens a file from disk and registers it for upload.
	// It adds the file to the openedFiles array and creates a corresponding progress bar.
	// For database dumps (!isSummary), SQL filtering is enabled based on args.filterSQL.
	addFile := func(filePath string) error {

		isSummary := strings.HasSuffix(filePath, summaryFile)

		// Open the file
		openFile, err := os.Open(filePath)

		if err != nil {
			if isSummary {
				return errors.Wrap(err, fmt.Sprintf("failed to open snapshot summary %s - Please generate it with 'src snapshot summary'", filePath))
			}
			return errors.Wrap(err, fmt.Sprintf("failed to open database dump %s - Please generate them with 'src snapshot databases'", filePath))
		}

		// Get file metadata (name, size)
		stat, err := openFile.Stat()
		if err != nil {
			return errors.Wrap(err, "get file size")
		}

		// Register file for upload
		openedFiles = append(openedFiles, uploadFile{
			file:      openFile,
			stat:      stat,
			filterSQL: !isSummary && args.filterSQL, // Only filter SQL for database dumps
		})

		// Create progress bar for this file
		progressBars = append(progressBars, output.ProgressBar{
			Label: stat.Name(),
			Max:   float64(stat.Size()),
		})
		return nil
	}

	// Open files based on user's selection (via --file arg)
	// Iterate through the user's selected files and open each one
	for _, selectedFile := range args.filesToUpload {

		// Construct full file path
		filePath := filepath.Join(srcSnapshotDir, selectedFile)

		if err := addFile(filePath); err != nil {
			return nil, nil, err
		}
	}

	return openedFiles, progressBars, nil
}

// uploadFilesToBucket uploads the prepared files to Google Cloud Storage bucket.
// Uploads are performed in parallel with progress tracking.
func uploadFilesToBucket(client *gcsClient, args *uploadArgs, openedFiles []uploadFile, progressBars []output.ProgressBar) error {

	// Start uploads with progress tracking
	progress := client.out.Progress(progressBars, nil)
	progress.WriteLine(output.Emoji(output.EmojiHourglass, "Starting uploads..."))
	bucket := client.storageClient.Bucket(args.bucketName)
	uploadPool := pool.New().WithErrors().WithContext(client.ctx)

	// Upload each file in parallel
	for fileIndex, openedFile := range openedFiles {

		fileIndex := fileIndex
		openedFile := openedFile

		uploadPool.Go(func(ctx context.Context) error {
			progressFn := func(bytesWritten int64) { progress.SetValue(fileIndex, float64(bytesWritten)) }

			if err := streamFileToBucket(ctx, &openedFile, bucket, progressFn); err != nil {
				return errors.Wrap(err, openedFile.stat.Name())
			}

			return nil
		})
	}

	// Wait for all uploads to complete
	errs := uploadPool.Wait()
	progress.Complete()
	if errs != nil {
		client.out.WriteLine(output.Line(output.EmojiFailure, output.StyleFailure, "Some file(s) failed to upload."))
		return errs
	}

	client.out.WriteLine(output.Emoji(output.EmojiSuccess, "File(s) uploaded successfully!"))
	return nil
}

func streamFileToBucket(ctx context.Context, file *uploadFile, bucket *storage.BucketHandle, progressFn func(int64)) error {

	// Set up GCS writer for the destination file
	writer := bucket.Object(file.stat.Name()).NewWriter(ctx)
	writer.ProgressFunc = progressFn
	defer writer.Close()

	// To assert against actual file size
	var totalBytesWritten int64

	// Start a partial copy, that filters out incompatible statements
	if file.filterSQL {
		bytesWritten, err := pgdump.FilterInvalidLines(writer, file.file, progressFn)
		if err != nil {
			return errors.Wrap(err, "filter out incompatible statements and upload")
		}
		totalBytesWritten += bytesWritten
	}

	// io.Copy is the best way to copy from a reader to writer in Go,
	// storage.Writer has its own chunking mechanisms internally.
	// io.Reader is stateful, so this copy will just continue from where FilterInvalidLines left off, if used
	bytesWritten, err := io.Copy(writer, file.file)
	if err != nil {
		return errors.Wrap(err, "upload")
	}
	totalBytesWritten += bytesWritten

	// Progress is not called on completion of io.Copy,
	// so we call it manually after to update our pretty progress bars.
	progressFn(bytesWritten)

	// Validate we have sent all data.
	// FilterInvalidLines may add some bytes, so the check is not a strict equality.
	fileSize := file.stat.Size()
	if totalBytesWritten < fileSize {
		return errors.Newf("expected to write %d bytes, but actually wrote %d bytes (diff: %d bytes)",
			fileSize, totalBytesWritten, totalBytesWritten-fileSize)
	}

	return nil
}
