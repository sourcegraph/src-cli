package main

import (
	"context"
	"flag"
	"fmt"
	"io"
	"os"
	"path"
	"path/filepath"
	"strings"

	"cloud.google.com/go/storage"
	"github.com/sourcegraph/conc/pool"
	"github.com/sourcegraph/sourcegraph/lib/errors"
	"github.com/sourcegraph/sourcegraph/lib/output"
	"google.golang.org/api/option"

	"github.com/sourcegraph/src-cli/internal/pgdump"
)

const srcSnapshotDir = "./src-snapshot"

var srcSnapshotSummaryPath = path.Join(srcSnapshotDir, "summary.json")

// Define types
type uploadArgs struct {
	bucketName      string
	credentialsPath string
	filterSQL       bool
	filesToUpload   []string
}

type gcsClient struct {
	ctx           context.Context
	out           *output.Output
	storageClient *storage.Client
}

// uploadFile represents a file registered for upload
type uploadFile struct {
	file      *os.File
	stat      os.FileInfo
	filterSQL bool // Whether to filter incompatible SQL statements during upload, true for database files, false for summary file
}


func init() {
	usage := `'src snapshot upload' uploads instance snapshot contents generated by 'src snapshot databases' and 'src snapshot summary' to the designated bucket.

USAGE
	src snapshot upload -bucket=$MIGRATION_BUCKET_NAME -credentials=./migration_private_key.json [-file=$FILE]

BUCKET
	In general, a Google Cloud Storage bucket and relevant credentials will be provided by Sourcegraph when using this functionality to share a snapshot with Sourcegraph.

FILE
	Optional: Specify files to upload as a comma-delimited list. Valid values: summary, primary, codeintel, codeinsights. Default: All valid values.
	Examples: -file=summary,primary or -file=codeintel
`

	flagSet := flag.NewFlagSet("upload", flag.ExitOnError)
	bucketName := flagSet.String("bucket", "", "destination Cloud Storage bucket name")
	credentialsPath := flagSet.String("credentials", "", "JSON credentials file for Google Cloud service account")
	filterSQL := flagSet.Bool("filter-sql", true, "filter incompatible SQL statements from database dumps for import to Google Cloud SQL")
	fileFilter := flagSet.String("file", "summary,primary,codeintel,codeinsights", "comma-delimited list of files to upload")

	// Register this command with the parent 'src snapshot' command.
	// The parent snapshot.go command runs all registered subcommands via snapshotCommands.run().
	// This self-registration pattern allows subcommands to automatically register themselves
	// when their init() functions run, without requiring a central registry file.
	snapshotCommands = append(snapshotCommands, &command{
		flagSet:   flagSet,
		handler:   snapshotUploadHandler(flagSet, bucketName, credentialsPath, filterSQL, fileFilter),
		usageFunc: func() { fmt.Fprint(flag.CommandLine.Output(), usage) },
	})
}

// Helper function to keep init() succinct
func snapshotUploadHandler(flagSet *flag.FlagSet, bucketName, credentialsPath *string, filterSQL *bool, fileFilter *string) func([]string) error {
	return func(args []string) error {
		if err := flagSet.Parse(args); err != nil {
			return err
		}

		// Validate and parse inputs
		uploadArgs, err := validateUploadInputs(*bucketName, *credentialsPath, *fileFilter, *filterSQL)
		if err != nil {
			return err
		}

		// Create client
		client, err := createGcsClient(flagSet, uploadArgs.credentialsPath)
		if err != nil {
			return err
		}

		// Open files and create progress bars
		openedFiles, progressBars, err := openFilesAndCreateProgressBars(uploadArgs)
		if err != nil {
			return err
		}

		// Upload files to bucket
		return uploadFilesToBucket(client, uploadArgs, openedFiles, progressBars)
	}
}

// Validate user inputs, and convert them to an object of type uploadArgs
func validateUploadInputs(bucketName, credentialsPath, fileFilter string, filterSQL bool) (*uploadArgs, error) {
	if bucketName == "" {
		return nil, errors.New("-bucket required")
	}
	if credentialsPath == "" {
		return nil, errors.New("-credentials required")
	}

	filesToUpload, err := parseFileFilter(fileFilter)
	if err != nil {
		return nil, err
	}

	return &uploadArgs{
		bucketName:      bucketName,
		credentialsPath: credentialsPath,
		filterSQL:       filterSQL,
		filesToUpload:   filesToUpload,
	}, nil
}

// Parse the --file arg values, and return a list of strings of the files to upload
func parseFileFilter(fileFilter string) ([]string, error) {

	validFiles := map[string]bool{
		"summary":      true,
		"primary":      true,
		"codeintel":    true,
		"codeinsights": true,
	}

	if fileFilter == "" {
		// Default: all files
		return []string{"summary", "primary", "codeintel", "codeinsights"}, nil
	}

	// Parse comma-delimited list
	var filesToUpload []string
	parts := strings.Split(fileFilter, ",")
	for _, part := range parts {

		// Normalize: trim spaces and strip file extensions
		normalized := strings.TrimSpace(part)
		normalized = strings.TrimSuffix(normalized, ".json")
		normalized = strings.TrimSuffix(normalized, ".sql")

		// Validate
		if !validFiles[normalized] {
			return nil, errors.Newf("invalid -file value %q. Valid values: summary[.json], primary[.sql], codeintel[.sql], codeinsights[.sql]", part)
		}

		filesToUpload = append(filesToUpload, normalized)
	}

	return filesToUpload, nil
}

func createGcsClient(flagSet *flag.FlagSet, credentialsPath string) (*gcsClient, error) {

	out := output.NewOutput(flagSet.Output(), output.OutputOpts{Verbose: *verbose})
	ctx := context.Background()
	// https://pkg.go.dev/cloud.google.com/go/storage#section-readme
	client, err := storage.NewClient(ctx, option.WithCredentialsFile(credentialsPath))

	if err != nil {
		return nil, errors.Wrap(err, "create Google Cloud Storage client")
	}

	return &gcsClient{
		ctx:           ctx,
		out:           out,
		storageClient: client,
	}, nil
}

// openFilesAndCreateProgressBars opens selected snapshot files from disk and creates progress bars for UI display.
// Returns arrays of uploadFile and progress bars (aligned by index).
func openFilesAndCreateProgressBars(args *uploadArgs) ([]uploadFile, []output.ProgressBar, error) {
	var (
		openedFiles  []uploadFile          // Files opened from disk, ready for upload (aligned with progressBars)
		progressBars []output.ProgressBar // Progress bars for UI (aligned with openedFiles)
	)

	// addFile opens a file from disk and registers it for upload.
	// It adds the file to the openedFiles array and creates a corresponding progress bar.
	// For database dumps (!isSummary), SQL filtering is enabled based on args.filterSQL.
	addFile := func(filePath string, isSummary bool) error {

		// Open the file
		openFile, err := os.Open(filePath)
		if err != nil {
			if isSummary {
				return errors.Wrap(err, "failed to open snapshot summary - generate one with 'src snapshot summary'")
			}
			return errors.Wrap(err, "failed to open database dump - generate one with 'src snapshot databases'")
		}

		// Get file metadata (name, size)
		stat, err := openFile.Stat()
		if err != nil {
			return errors.Wrap(err, "get file size")
		}

		// Register file for upload
		openedFiles = append(openedFiles, uploadFile{
			file:      openFile,
			stat:      stat,
			filterSQL: !isSummary && args.filterSQL, // Only filter SQL for database dumps
		})

		// Create progress bar for this file
		progressBars = append(progressBars, output.ProgressBar{
			Label: stat.Name(),
			Max:   float64(stat.Size()),
		})
		return nil
	}

	// Open files based on user's selection (via --file arg)
	// Iterate through the user's selected files and open each one
	for _, selectedFile := range args.filesToUpload {
		if selectedFile == "summary" {
			// Open summary.json
			if err := addFile(srcSnapshotSummaryPath, true); err != nil {
				return nil, nil, err
			}
		} else {
			// Open database dump file (primary.sql, codeintel.sql, or codeinsights.sql)
			dbFilePath := filepath.Join(srcSnapshotDir, selectedFile+".sql")
			if err := addFile(dbFilePath, false); err != nil {
				return nil, nil, err
			}
		}
	}

	return openedFiles, progressBars, nil
}

// uploadFilesToBucket uploads the prepared files to Google Cloud Storage bucket.
// Uploads are performed in parallel with progress tracking.
func uploadFilesToBucket(client *gcsClient, args *uploadArgs, openedFiles []uploadFile, progressBars []output.ProgressBar) error {
	// Start uploads with progress tracking
	progress := client.out.Progress(progressBars, nil)
	progress.WriteLine(output.Emoji(output.EmojiHourglass, "Starting uploads..."))
	bucket := client.storageClient.Bucket(args.bucketName)
	uploadPool := pool.New().WithErrors().WithContext(client.ctx)

	// Upload each file in parallel
	for fileIndex, openedFile := range openedFiles {
		fileIndex := fileIndex
		openedFile := openedFile
		uploadPool.Go(func(ctx context.Context) error {
			progressFn := func(bytesWritten int64) { progress.SetValue(fileIndex, float64(bytesWritten)) }

			if err := streamFileToBucket(ctx, &openedFile, bucket, progressFn); err != nil {
				return errors.Wrap(err, openedFile.stat.Name())
			}

			return nil
		})
	}

	// Wait for all uploads to complete
	errs := uploadPool.Wait()
	progress.Complete()
	if errs != nil {
		client.out.WriteLine(output.Line(output.EmojiFailure, output.StyleFailure, "Some snapshot contents failed to upload."))
		return errs
	}

	client.out.WriteLine(output.Emoji(output.EmojiSuccess, "Summary contents uploaded!"))
	return nil
}

func streamFileToBucket(ctx context.Context, file *uploadFile, bucket *storage.BucketHandle, progressFn func(int64)) error {

	// Set up GCS writer for the destination file
	writer := bucket.Object(file.stat.Name()).NewWriter(ctx)
	writer.ProgressFunc = progressFn
	defer writer.Close()

	// To assert against actual file size
	var totalBytesWritten int64

	// Do a partial copy, that filters out incompatible statements
	if file.filterSQL {
		bytesWritten, err := pgdump.FilterInvalidLines(writer, file.file, progressFn)
		if err != nil {
			return errors.Wrap(err, "filter out incompatible statements and upload")
		}
		totalBytesWritten += bytesWritten
	}

	// io.Copy is the best way to copy from a reader to writer in Go,
	// storage.Writer has its own chunking mechanisms internally.
	// io.Reader is stateful, so this copy will just continue from where FilterInvalidLines left off, if used
	bytesWritten, err := io.Copy(writer, file.file)
	if err != nil {
		return errors.Wrap(err, "upload")
	}
	totalBytesWritten += bytesWritten

	// Progress is not called on completion of io.Copy,
	// so we call it manually after to update our pretty progress bars.
	progressFn(bytesWritten)

	// Validate we have sent all data.
	// FilterInvalidLines may add some bytes, so the check is not a strict equality.
	fileSize := file.stat.Size()
	if totalBytesWritten < fileSize {
		return errors.Newf("expected to write %d bytes, but actually wrote %d bytes (diff: %d bytes)",
			fileSize, totalBytesWritten, totalBytesWritten-fileSize)
	}

	return nil
}
